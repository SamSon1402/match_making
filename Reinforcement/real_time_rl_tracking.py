# -*- coding: utf-8 -*-
"""Real_Time_RL_Tracking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XbaZvLhWx_QvRa7j_MDim556GN698Xjy
"""

import tensorflow as tf
import numpy as np

# Define the recommendation environment
class RecommendationEnv:
    def __init__(self, num_users, num_items):
        self.num_users = num_users
        self.num_items = num_items

    def step(self, action):
        # Simulate user feedback (1 for like, 0 for dislike)
        reward = np.random.choice([0, 1], p=[0.7, 0.3])  # 30% chance of liking the recommendation
        next_state = np.random.randint(0, self.num_users)
        return next_state, reward

# Create the environment
num_users = 1000
num_items = 100
env = RecommendationEnv(num_users, num_items)

# Define the Q-network
model = tf.keras.Sequential([
    tf.keras.layers.Dense(100, activation='relu', input_shape=(1,)),
    tf.keras.layers.Dense(50, activation='relu'),
    tf.keras.layers.Dense(num_items)
])

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
              loss='mse')

# Training loop
num_iterations = 20000
batch_size = 32
replay_buffer = []

for _ in range(num_iterations):
    state = np.random.randint(0, num_users)
    action = np.random.randint(0, num_items)
    next_state, reward = env.step(action)

    replay_buffer.append((state, action, reward, next_state))
    if len(replay_buffer) > 1000:
        replay_buffer.pop(0)

    if len(replay_buffer) >= batch_size:
        batch = np.random.choice(len(replay_buffer), batch_size, replace=False)
        states = np.array([replay_buffer[i][0] for i in batch])
        actions = np.array([replay_buffer[i][1] for i in batch])
        rewards = np.array([replay_buffer[i][2] for i in batch])
        next_states = np.array([replay_buffer[i][3] for i in batch])

        current_q = model.predict(states)
        next_q = model.predict(next_states)
        max_next_q = np.max(next_q, axis=1)
        current_q[np.arange(batch_size), actions] = rewards + 0.99 * max_next_q

        model.fit(states, current_q, verbose=0)

# Function to get recommendations
def get_recommendation(user_id):
    q_values = model.predict(np.array([user_id]))
    return np.argmax(q_values[0])

# Example usage
user_id = 42
recommended_item = get_recommendation(user_id)
print(f"Recommended item for user {user_id}: {recommended_item}")

# Test the system
def test_recommendation_system(num_test_users=100, num_recommendations=10):
    user_item_matrix = {}

    for user in range(num_test_users):
        user_item_matrix[user] = []
        for _ in range(num_recommendations):
            recommended_item = get_recommendation(user)
            user_item_matrix[user].append(recommended_item)

    return user_item_matrix

# Analyze test results
def analyze_results(user_item_matrix):
    total_users = len(user_item_matrix)
    total_recommendations = sum(len(items) for items in user_item_matrix.values())
    unique_recommendations = len(set(item for items in user_item_matrix.values() for item in items))

    avg_unique_per_user = np.mean([len(set(items)) for items in user_item_matrix.values()])

    print(f"Total users tested: {total_users}")
    print(f"Total recommendations made: {total_recommendations}")
    print(f"Unique items recommended: {unique_recommendations}")
    print(f"Average unique items per user: {avg_unique_per_user:.2f}")

    # Calculate recommendation frequency
    item_frequency = {}
    for items in user_item_matrix.values():
        for item in items:
            item_frequency[item] = item_frequency.get(item, 0) + 1

    top_items = sorted(item_frequency.items(), key=lambda x: x[1], reverse=True)[:10]
    print("\nTop 10 most recommended items:")
    for item, freq in top_items:
        print(f"Item {item}: Recommended {freq} times")

# Run the test
test_results = test_recommendation_system()
analyze_results(test_results)